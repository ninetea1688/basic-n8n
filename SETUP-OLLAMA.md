# ü§ñ Ollama + n8n Integration Guide

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏≤‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Ollama ‡πÅ‡∏•‡∏∞ n8n ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô AI Models ‡πÉ‡∏ô‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô (Local) ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ External APIs ‡πÄ‡∏ä‡πà‡∏ô OpenAI

## ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Ollama
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
- **‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢**: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢ API fees
- **‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡∏µ‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
- **‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ Models**: Llama, Mistral, CodeLlama, ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß**: Response time ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

## Models ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö

### Llama Models
- `llama3.2` (3B, 1B) - ‡∏£‡∏∏‡πà‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
- `llama3.1` (8B, 70B, 405B) - ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
- `llama2` (7B, 13B, 70B) - ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£, ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß

### Code Models
- `codellama` (7B, 13B, 34B) - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î
- `deepseek-coder` - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### Thai Language Models
- `typhoon` - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©
- `seallm` - Southeast Asian Languages

### Lightweight Models
- `phi3` (3.8B) - ‡πÄ‡∏•‡πá‡∏Å ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
- `gemma` (2B, 7B) - ‡∏à‡∏≤‡∏Å Google
- `qwen2` (0.5B, 1.5B, 7B) - ‡∏à‡∏≤‡∏Å Alibaba

## ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏ö

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
- **RAM**: 8GB (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö models 3B-7B)
- **Storage**: 10GB ‡∏ß‡πà‡∏≤‡∏á
- **CPU**: x64 ‡∏´‡∏£‡∏∑‡∏≠ ARM64

### ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
- **RAM**: 16GB+ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö models 13B+)
- **GPU**: NVIDIA GPU ‡∏û‡∏£‡πâ‡∏≠‡∏° CUDA (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CPU 10-50 ‡πÄ‡∏ó‡πà‡∏≤)
- **Storage**: SSD 50GB+

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production
- **RAM**: 32GB+
- **GPU**: RTX 4090, A100, ‡∏´‡∏£‡∏∑‡∏≠ H100
- **Storage**: NVMe SSD 100GB+

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Services
```bash
# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Docker Compose (‡∏£‡∏ß‡∏° Ollama)
cd /Users/ninetea/Projects/nhso/n8n-basic
docker compose up -d

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
docker compose ps
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥
curl http://localhost:11434/api/tags

# ‡∏î‡∏π Models ‡∏ó‡∏µ‡πà‡∏°‡∏µ
docker exec n8n-ollama ollama list
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Models ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
```bash
# Llama 3.2 (3B) - ‡πÄ‡∏£‡πá‡∏ß, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
docker exec n8n-ollama ollama pull llama3.2:3b

# Llama 3.1 (8B) - ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
docker exec n8n-ollama ollama pull llama3.1:8b

# CodeLlama (7B) - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î
docker exec n8n-ollama ollama pull codellama:7b

# Phi-3 (3.8B) - ‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
docker exec n8n-ollama ollama pull phi3:3.8b

# Gemma (7B) - ‡∏à‡∏≤‡∏Å Google
docker exec n8n-ollama ollama pull gemma:7b
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: Import Workflow
1. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà n8n (http://localhost:5678)
2. Import ‡πÑ‡∏ü‡∏•‡πå `ollama-ai-workflow.json`
3. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Workflow

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Chat API
curl -X POST http://localhost:5678/webhook/ollama-chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?",
    "system_prompt": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
  }'
```

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô n8n Workflows

### 1. HTTP Request Node
```json
{
  "url": "http://ollama:11434/api/generate",
  "method": "POST",
  "body": {
    "model": "llama3.2",
    "prompt": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "top_p": 0.9,
      "max_tokens": 1000
    }
  }
}
```

### 2. Chat API (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
```json
{
  "url": "http://ollama:11434/api/chat",
  "method": "POST",
  "body": {
    "model": "llama3.2",
    "messages": [
      {
        "role": "system",
        "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£"
      },
      {
        "role": "user",
        "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ"
      }
    ],
    "stream": false
  }
}
```

### 3. Code Node ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
```javascript
// ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ollama
const userMessage = $json.message || '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ';
const systemPrompt = $json.system_prompt || '‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢';

return {
  model: 'llama3.2',
  messages: [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userMessage }
  ],
  stream: false,
  options: {
    temperature: 0.7,
    top_p: 0.9,
    max_tokens: 1000,
    stop: ['Human:', 'Assistant:']
  }
};
```

## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Parameters

### Temperature (0.0 - 2.0)
- `0.0-0.3`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á
- `0.4-0.7`: ‡∏™‡∏°‡∏î‡∏∏‡∏•, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
- `0.8-1.2`: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå, ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
- `1.3-2.0`: ‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡∏Å, ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏õ‡∏•‡∏Å‡πÜ

### Top-p (0.0 - 1.0)
- `0.1-0.3`: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
- `0.4-0.7`: ‡∏™‡∏°‡∏î‡∏∏‡∏•
- `0.8-0.95`: ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
- `1.0`: ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î

### Max Tokens
- `100-300`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô
- `500-1000`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
- `1500-2000`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß
- `4000+`: ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏¢‡∏≤‡∏ß

### Stop Sequences
```javascript
"stop": ["Human:", "Assistant:", "\n\n", "---"]
```

## Use Cases ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

### 1. Customer Support Chatbot
```javascript
// System Prompt
const systemPrompt = `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ù‡πà‡∏≤‡∏¢‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:
1. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
2. ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤
3. ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
4. ‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÄ‡∏™‡∏°‡∏≠`;
```

### 2. Content Generator
```javascript
// ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Blog
const prompt = `‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö "${topic}" ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 500 ‡∏Ñ‡∏≥ 
‡∏£‡∏ß‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ 3 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°`;
```

### 3. Code Assistant
```javascript
// ‡πÉ‡∏ä‡πâ CodeLlama
const codePrompt = `‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
${requirement}

‡∏£‡∏ß‡∏°:
- Docstring
- Type hints
- Error handling
- Unit tests`;
```

### 4. Data Analysis
```javascript
// ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets
const analysisPrompt = `‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
${JSON.stringify(data)}

‡πÉ‡∏´‡πâ:
1. ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
2. ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
3. ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞`;
```

### 5. Translation Service
```javascript
// ‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤
const translatePrompt = `‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©:
"${thaiText}"

‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á`;
```

## ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Models

### ‡∏î‡∏π Models ‡∏ó‡∏µ‡πà‡∏°‡∏µ
```bash
docker exec n8n-ollama ollama list
```

### ‡∏•‡∏ö Models
```bash
# ‡∏•‡∏ö model ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
docker exec n8n-ollama ollama rm llama2:7b
```

### ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Models
```bash
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
docker exec n8n-ollama ollama pull llama3.2:latest
```

### ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Model
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Modelfile
echo 'FROM llama3.2
SYSTEM "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô"' > Modelfile

# ‡∏™‡∏£‡πâ‡∏≤‡∏á custom model
docker exec -i n8n-ollama ollama create finance-assistant < Modelfile
```

## ‡∏Å‡∏≤‡∏£ Monitor ‡πÅ‡∏•‡∏∞ Debug

### ‡∏î‡∏π Logs
```bash
# ‡∏î‡∏π logs ‡∏Ç‡∏≠‡∏á Ollama
docker logs n8n-ollama

# ‡∏î‡∏π logs ‡πÅ‡∏ö‡∏ö real-time
docker logs -f n8n-ollama
```

### ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Performance
```bash
# ‡∏î‡∏π resource usage
docker stats n8n-ollama

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• model
curl http://localhost:11434/api/show -d '{
  "name": "llama3.2"
}'
```

### Debug Workflow
```javascript
// ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô Code Node ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
console.log('Request to Ollama:', JSON.stringify($json, null, 2));

// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö response
if (!$json.response) {
  throw new Error('No response from Ollama');
}

console.log('Ollama Response:', $json.response);
```

## ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### Ollama ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ container
docker ps | grep ollama

# ‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó Ollama
docker restart n8n-ollama

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs
docker logs n8n-ollama
```

### Model ‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà disk
df -h

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö RAM
free -h

# ‡∏•‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà
docker exec n8n-ollama ollama pull llama3.2
```

### Response ‡∏ä‡πâ‡∏≤
1. **‡∏•‡∏î Model Size**: ‡πÉ‡∏ä‡πâ 3B ‡πÅ‡∏ó‡∏ô 7B
2. **‡∏•‡∏î Max Tokens**: ‡∏à‡∏≤‡∏Å 2000 ‡πÄ‡∏õ‡πá‡∏ô 500
3. **‡πÄ‡∏û‡∏¥‡πà‡∏° RAM**: ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 16GB
4. **‡πÉ‡∏ä‡πâ GPU**: ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CPU ‡∏°‡∏≤‡∏Å

### Out of Memory
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory usage
docker stats

# ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤
docker exec n8n-ollama ollama pull llama3.2:1b
```

## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### GPU Support (NVIDIA)
```yaml
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô docker-compose.yml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

### Custom Environment Variables
```yaml
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô docker-compose.yml
environment:
  - OLLAMA_HOST=0.0.0.0
  - OLLAMA_ORIGINS=*
  - OLLAMA_NUM_PARALLEL=4
  - OLLAMA_MAX_LOADED_MODELS=2
  - OLLAMA_FLASH_ATTENTION=1
```

### Load Balancing
```yaml
# ‡∏´‡∏•‡∏≤‡∏¢ Ollama instances
ollama-1:
  image: ollama/ollama:latest
  ports: ["11434:11434"]
  
ollama-2:
  image: ollama/ollama:latest
  ports: ["11435:11434"]
```

### Persistent Models
```yaml
# ‡πÄ‡∏Å‡πá‡∏ö models ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô host
volumes:
  - ./ollama-models:/root/.ollama
```

## Security Best Practices

### Network Security
```yaml
# ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á
ollama:
  ports:
    - "127.0.0.1:11434:11434"  # localhost only
```

### API Authentication
```javascript
// ‡πÄ‡∏û‡∏¥‡πà‡∏° API key check ‡πÉ‡∏ô n8n
const apiKey = $json.headers['x-api-key'];
if (apiKey !== process.env.OLLAMA_API_KEY) {
  throw new Error('Unauthorized');
}
```

### Rate Limiting
```javascript
// ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô requests
const rateLimiter = global.get('rateLimiter') || {};
const clientIP = $json.headers['x-forwarded-for'] || 'unknown';
const now = Date.now();

if (rateLimiter[clientIP] && now - rateLimiter[clientIP] < 5000) {
  throw new Error('Rate limit exceeded');
}

rateLimiter[clientIP] = now;
global.set('rateLimiter', rateLimiter);
```

## Performance Optimization

### Model Selection Strategy
```javascript
// ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô
function selectModel(taskComplexity, responseTime) {
  if (taskComplexity === 'simple' && responseTime === 'fast') {
    return 'llama3.2:1b';
  } else if (taskComplexity === 'medium') {
    return 'llama3.2:3b';
  } else if (taskComplexity === 'complex') {
    return 'llama3.1:8b';
  }
  return 'llama3.2';
}
```

### Caching Responses
```javascript
// Cache ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
const cacheKey = require('crypto')
  .createHash('md5')
  .update(JSON.stringify($json))
  .digest('hex');

const cached = global.get(`cache_${cacheKey}`);
if (cached && Date.now() - cached.timestamp < 3600000) { // 1 hour
  return cached.response;
}
```

### Batch Processing
```javascript
// ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
const questions = $json.questions || [];
const promises = questions.map(async (question) => {
  // Call Ollama API
  return await callOllama(question);
});

const responses = await Promise.all(promises);
```

## Monitoring ‡πÅ‡∏•‡∏∞ Analytics

### Usage Metrics
```javascript
// ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
const stats = global.get('ollamaStats') || {
  totalRequests: 0,
  totalTokens: 0,
  averageResponseTime: 0,
  modelUsage: {}
};

stats.totalRequests++;
stats.modelUsage[$json.model] = (stats.modelUsage[$json.model] || 0) + 1;

global.set('ollamaStats', stats);
```

### Health Check
```javascript
// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á Ollama
const healthCheck = async () => {
  try {
    const response = await fetch('http://ollama:11434/api/tags');
    return response.ok;
  } catch (error) {
    return false;
  }
};
```

## ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏£‡∏∞‡∏ö‡∏ö

### Microservices Architecture
```yaml
# ‡πÅ‡∏¢‡∏Å services
services:
  ollama-chat:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_MODELS=llama3.2,phi3
      
  ollama-code:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_MODELS=codellama,deepseek-coder
      
  ollama-translate:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_MODELS=llama3.1:8b
```

### Auto Scaling
```bash
# ‡πÉ‡∏ä‡πâ Docker Swarm ‡∏´‡∏£‡∏∑‡∏≠ Kubernetes
docker service create \
  --name ollama-cluster \
  --replicas 3 \
  --publish 11434:11434 \
  ollama/ollama:latest
```

---

## ‡∏™‡∏£‡∏∏‡∏õ

Ollama + n8n ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á AI Workflows ‡∏ó‡∏µ‡πà:
- **‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
- **‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î**: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ API
- **‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô**: ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
- **‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£**: ‡πÑ‡∏°‡πà‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ External Services

‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏π‡∏á